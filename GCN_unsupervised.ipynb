{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJlXOIqzCWUa",
        "outputId": "6639bc3b-5666-4ada-d211-d43e3b8b1252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "1.12.1+cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pIWcaJfCwsa",
        "outputId": "5a012e6e-0204-4e1e-83c7-f2e753d73f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
            "\u001b[K     |████████████████████████████████| 467 kB 14.0 MB/s \n",
            "\u001b[?25hCollecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.1.0%2Bpt112cu113-cp37-cp37m-linux_x86_64.whl (8.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9 MB 14.1 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.15%2Bpt112cu113-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 67.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=7e65d66c004b92be64cf18166c7fca18d90efbcffa16b96ea029c65630b878e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-sparse, torch-scatter, torch-geometric\n",
            "Successfully installed torch-geometric-2.1.0.post1 torch-scatter-2.1.0+pt112cu113 torch-sparse-0.6.15+pt112cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "import random\n",
        "import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch_geometric import nn as gnn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "from torch import optim\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "FSZSzx4AK5or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = \"drive/MyDrive/kg_Data\"\n",
        "\n",
        "\n",
        "def get_data(file_path, convert_to_dict = True):\n",
        "  rows = []\n",
        "  with open(file_path) as csvf:\n",
        "    reader = csv.DictReader(csvf)\n",
        "    for row in reader:\n",
        "      rows.append(row)\n",
        "    if convert_to_dict:\n",
        "      rows = {row['name']: row for row in rows}\n",
        "    return rows, reader.fieldnames\n",
        "\n",
        "\n",
        "def get_relation_data(file_path, source_nodes, dest_nodes):\n",
        "  rows, fieldnames = get_data(file_path, convert_to_dict=False)\n",
        "  relations = [(row[fieldnames[0]], row[fieldnames[2]]) for row in rows]\n",
        "  relations = [rel for rel in relations if \n",
        "               ((rel[0] in source_nodes) and (rel[1] in dest_nodes))]\n",
        "  return relations\n",
        "\n",
        "anatomy, _ = get_data(os.path.join(root_path, \"anatomy.csv\"))\n",
        "equipment, _ = get_data(os.path.join(root_path, \"equipment.csv\"))\n",
        "exercise, _ = get_data(os.path.join(root_path, \"exercise.csv\"))\n",
        "muscles, _ = get_data(os.path.join(root_path, \"muscles.csv\"))\n",
        "yoga, _ = get_data(os.path.join(root_path, \"yoga.csv\"))\n",
        "\n",
        "muscle_and_anatomy = deepcopy(muscles)\n",
        "muscle_and_anatomy.update(anatomy)\n",
        "\n",
        "exercise_and_yoga = deepcopy(exercise)\n",
        "exercise_and_yoga.update(yoga)\n",
        "\n",
        "follows = get_relation_data(os.path.join(root_path, \"rel_FOLLOWS_yoga.csv\"), yoga, yoga)\n",
        "preceds = get_relation_data(os.path.join(root_path, \"rel_PRECEDS_yoga.csv\"), yoga, yoga)\n",
        "targets = get_relation_data(os.path.join(root_path, \"rel_TARGETS.csv\"), exercise, muscle_and_anatomy)\n",
        "targets_yoga = get_relation_data(os.path.join(root_path, \"rel_TARGETS_yoga.csv\"), yoga, muscle_and_anatomy)\n",
        "uses = get_relation_data(os.path.join(root_path, \"rel_USES.csv\"), exercise, equipment)\n",
        "part_of = get_relation_data(os.path.join(root_path, \"rel_PART_OF.csv\"), muscle_and_anatomy, muscle_and_anatomy)"
      ],
      "metadata": {
        "id": "I02vfnYGCy2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(follows), len(preceds), len(targets), len(targets_yoga), len(uses), len(part_of)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIKTxag7Hw9K",
        "outputId": "7e6cbea2-542f-4d3f-f55c-5704d8178724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(175, 179, 3905, 478, 1424, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns = {\n",
        "    'anatomy': [['name']],\n",
        "    'equipment': [['name']],\n",
        "    'exercise': [['name'], ['difficulty', 'force_type', 'mechanics', 'type']],\n",
        "    'muscles': [['name']],\n",
        "    'yoga': [['name']]\n",
        "}\n",
        "\n",
        "def convert_to_list(node_maps):\n",
        "  result = {}\n",
        "  feature_encoders = {}\n",
        "  for k, values in node_maps.items():\n",
        "    curr_list = []\n",
        "    for value in values.values():\n",
        "      curr_value = []\n",
        "      for col in columns[k][0]:\n",
        "        curr_value.append(value[col])\n",
        "      curr_value2 = []\n",
        "      if len(columns[k]) > 1:\n",
        "        for col in columns[k][1]:\n",
        "          curr_value2.append(value[col])\n",
        "      curr_list.append([curr_value[0], curr_value2])\n",
        "    curr_list.sort(key = lambda x: x[0])\n",
        "    curr_list1, curr_list2 = zip(*curr_list)\n",
        "    feature_encoders[k] = [LabelEncoder()]\n",
        "    result[k] = [torch.Tensor(feature_encoders[k][0].fit_transform(curr_list1)).unsqueeze(-1)]\n",
        "    if len(columns[k]) > 1:\n",
        "      feature_encoders[k].append(OneHotEncoder(handle_unknown='ignore'))\n",
        "      result[k].append(torch.Tensor(feature_encoders[k][1].fit_transform(curr_list2).toarray()))\n",
        "    result[k] = torch.cat(result[k], dim=1)\n",
        "  return result, feature_encoders"
      ],
      "metadata": {
        "id": "uNwazoDdiqw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_type_map = {\n",
        "    'anatomy': anatomy,\n",
        "    'equipment': equipment,\n",
        "    'exercise': exercise,\n",
        "    'muscles': muscles,\n",
        "    'yoga': yoga\n",
        "}\n",
        "\n",
        "node_type_map_lookup = {}\n",
        "for key, value in node_type_map.items():\n",
        "  for node in value:\n",
        "    node_type_map_lookup[node] = key\n",
        "\n",
        "node_vectors, node_encoders = convert_to_list(node_type_map)\n",
        "\n",
        "def get_index_encoder(node_map):\n",
        "  nodes = [(node_type, node) for node_type, node_values in node_map.items() for node in node_values.keys()]\n",
        "  nodes.sort()\n",
        "  encoder = { node: index for index, (node_type, node) in enumerate(nodes)}\n",
        "  return encoder, [node[1] for node in nodes]\n",
        "index_encoder, index_decoder = get_index_encoder(node_type_map)"
      ],
      "metadata": {
        "id": "tteTACl2HwLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessor(nn.Module):\n",
        "  def __init__(self, shapes, emb_size=256):\n",
        "    super(Preprocessor, self).__init__()\n",
        "    self.embedding_layers = nn.ModuleList([nn.Embedding(num_rows, (emb_size) - num_features + 1) for _, num_rows,num_features in shapes])\n",
        "    self.nn_layers = nn.ModuleList([nn.Linear(emb_size, emb_size) for _ in shapes])\n",
        "  \n",
        "  def forward(self, nodes):\n",
        "    result = []\n",
        "    for i, node_vector in enumerate(nodes):\n",
        "      x = self.embedding_layers[i](node_vector[:, 0].long())\n",
        "      if node_vector.shape[1] > 1:\n",
        "        x = torch.cat([x, node_vector[:, 1:]], dim=1)\n",
        "      x = self.nn_layers[i](x)\n",
        "      result.append(x)\n",
        "    result = torch.cat(result, dim = 0)\n",
        "    return result"
      ],
      "metadata": {
        "id": "sZ7rbbWGmros"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_vector_shapes = [(k, value.shape[0], value.shape[1]) for k, value in node_vectors.items()]\n",
        "node_vector_shapes.sort()\n",
        "node_vector_list = [(k, value) for k, value in node_vectors.items()]\n",
        "node_vector_list.sort(key = lambda x: x[0])\n",
        "_, node_vector_list = zip(*node_vector_list)\n",
        "preprocessor = Preprocessor(node_vector_shapes, emb_size=256)\n",
        "for name, param in preprocessor.named_parameters():\n",
        "  print(name, param.shape)\n",
        "preprocessed_node_vectors = preprocessor(node_vector_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVbKvzw4wfMH",
        "outputId": "4051f6e2-ddf8-4813-d153-007b70a65537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding_layers.0.weight torch.Size([24, 256])\n",
            "embedding_layers.1.weight torch.Size([57, 256])\n",
            "embedding_layers.2.weight torch.Size([1006, 197])\n",
            "embedding_layers.3.weight torch.Size([49, 256])\n",
            "embedding_layers.4.weight torch.Size([92, 256])\n",
            "nn_layers.0.weight torch.Size([256, 256])\n",
            "nn_layers.0.bias torch.Size([256])\n",
            "nn_layers.1.weight torch.Size([256, 256])\n",
            "nn_layers.1.bias torch.Size([256])\n",
            "nn_layers.2.weight torch.Size([256, 256])\n",
            "nn_layers.2.bias torch.Size([256])\n",
            "nn_layers.3.weight torch.Size([256, 256])\n",
            "nn_layers.3.bias torch.Size([256])\n",
            "nn_layers.4.weight torch.Size([256, 256])\n",
            "nn_layers.4.bias torch.Size([256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verification\n",
        "all_node_vectors = [x for lis in node_vector_list for x in lis]\n",
        "index = random.randint(0, preprocessed_node_vectors.shape[0])\n",
        "name = node_encoders[node_type_map_lookup[index_decoder[index]]][0].inverse_transform(all_node_vectors[index][0:1].long().numpy())[0]\n",
        "assert(index_encoder[name] == index)\n",
        "print(index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_6efkdWzHRk",
        "outputId": "2ae8ee72-2bfd-4de0-cb7a-77c3a11a629a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relations = [follows, preceds, targets, targets_yoga, uses, part_of]\n",
        "edges = []\n",
        "for i, relation in enumerate(relations):\n",
        "  curr_edges = [[index_encoder[source], index_encoder[destination], i] for source, destination in relation]  \n",
        "  curr_edges = torch.Tensor(np.array(curr_edges))\n",
        "  edges.append(curr_edges)\n",
        "edges = torch.cat(edges, dim = 0)\n",
        "print(edges.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I24yCZ9m7Ucp",
        "outputId": "492fbc24-6838-4bfb-ff0d-26d524359cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6207, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_RELATION_TYPES = len(np.unique(edges[:, 2]))\n",
        "NUM_RELATIONS = edges.shape[0]\n",
        "NUM_NODES = preprocessed_node_vectors.shape[0]\n",
        "NODE_VECTOR_LENGTH = preprocessed_node_vectors.shape[1]"
      ],
      "metadata": {
        "id": "Nyen6tSh9Y3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNEncoder(nn.Module):\n",
        "  def __init__(self, layers):\n",
        "    super(GCNEncoder, self).__init__()\n",
        "    self.gcns = nn.ModuleList([gnn.RGCNConv(layers[i], layers[i+1], num_relations=NUM_RELATION_TYPES) for i in range(len(layers) - 1)])\n",
        "  \n",
        "  def forward(self, x, edges):\n",
        "    edge_indices = edges[:, :2].reshape((2, NUM_RELATIONS)).long()\n",
        "    edge_types = edges[:, 2].long()\n",
        "    num_gcns = len(self.gcns)\n",
        "    for i, gcn in enumerate(self.gcns):\n",
        "      x = gcn(x, edge_indices, edge_types)\n",
        "      if i < (num_gcns - 1):\n",
        "        x = F.relu(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "SXSGb4VH38xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [NODE_VECTOR_LENGTH, 512, 256, 128]\n",
        "gcn_encoder = GCNEncoder(layers)\n",
        "node_embeddings = gcn_encoder(preprocessed_node_vectors, edges)"
      ],
      "metadata": {
        "id": "XeRYdf1g_25j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_targets = targets + targets_yoga\n",
        "positive_uses = uses"
      ],
      "metadata": {
        "id": "P27dLdSeJoLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets_df = pd.DataFrame(positive_targets, columns=[\"exercise\", \"anatomy\"])\n",
        "targets_df.groupby(\"exercise\").agg({ \"anatomy\": \"count\"}).reset_index()[\"anatomy\"].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kNulYtEKw7Y",
        "outputId": "5280cb9c-0ec8-4b27-cb99-3ced6da8230f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.013736263736264"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uses_df = pd.DataFrame(positive_uses, columns=[\"exercise\", \"equipment\"])\n",
        "uses_df.groupby(\"exercise\").agg({ \"equipment\": \"count\"}).reset_index()[\"equipment\"].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8Ndix2TJpJ4",
        "outputId": "f1049729-b9ee-4b73-9ce9-119121e97e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.415506958250497"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_targets = []\n",
        "negative_uses = []\n",
        "for exercise in tqdm(exercise_and_yoga.keys()):\n",
        "  valid_muscles = [m for m in muscle_and_anatomy.keys() if (exercise, m) not in positive_targets]\n",
        "  for m in random.sample(valid_muscles, 5):\n",
        "    negative_targets.append((exercise, m))\n",
        "  valid_equipments = [m for m in equipment.keys() if (exercise, m) not in positive_uses]\n",
        "  for m in random.sample(valid_equipments, 2):\n",
        "    negative_uses.append((exercise, m))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maKo84T8Lpoh",
        "outputId": "bd4c33f7-03fc-4187-8d87-4ae9680bd789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1098/1098 [00:22<00:00, 47.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_pairs = [(index_encoder[t[0]], index_encoder[t[1]], 1) for t in positive_targets] + [(index_encoder[t[0]], index_encoder[t[1]], 0) for t in negative_targets]\n",
        "uses_pairs = [(index_encoder[t[0]], index_encoder[t[1]], 1) for t in positive_uses] + [(index_encoder[t[0]], index_encoder[t[1]], 0) for t in negative_uses]\n",
        "target_pairs = target_pairs + uses_pairs\n",
        "\n",
        "target_pairs = np.array(target_pairs)\n",
        "train_target_pair_x, test_target_pair_x, train_target_pair_y, test_target_pair_y = train_test_split(target_pairs[:,:2], target_pairs[:, 2], stratify=target_pairs[:, 2], test_size=0.2)"
      ],
      "metadata": {
        "id": "sCudagmgOOqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_target_pair_x.shape, train_target_pair_y.shape, test_target_pair_x.shape, test_target_pair_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIS-Xt8JOXin",
        "outputId": "5295fc76-5071-4def-8e0b-698e76b76993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10794, 2), (10794,), (2699, 2), (2699,))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining dataset class for dataloader\n",
        "class PairDataset(data.Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        assert(len(self.x) == len(self.y))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "\n",
        "dummy_dataset = PairDataset(train_target_pair_x,train_target_pair_y)\n",
        "dummy_dataloader = data.DataLoader(dummy_dataset, batch_size=32, shuffle=True)\n",
        "output , labels = next(iter(dummy_dataloader))\n",
        "print(output.shape)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0220z6HQ0cD",
        "outputId": "90240331-d15a-43f3-967c-50037375f0cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 2])\n",
            "tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
            "        0, 1, 0, 1, 0, 1, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_target_dataset = PairDataset(train_target_pair_x,train_target_pair_y)\n",
        "train_target_dataloader = data.DataLoader(train_target_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_target_dataset = PairDataset(test_target_pair_x,test_target_pair_y)\n",
        "test_target_dataloader = data.DataLoader(test_target_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "_R2xBOQXE8pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and evaluation functions\n",
        "def send_to_device(tensor):\n",
        "    if (type(tensor) == list) or (type(tensor) == tuple):\n",
        "        return [send_to_device(tenr) for tenr in tensor]\n",
        "    if (type(tensor) == dict):\n",
        "        return {k: send_to_device(v) for k, v in tensor.items()}\n",
        "    elif torch.is_tensor(tensor):\n",
        "        return tensor.to(\"cuda\")\n",
        "    else:\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def train_one_epoch(model, nodes, edges, train_data_loader, optimizer, loss_fn, scheduler):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "    pb = tqdm(enumerate(train_data_loader))\n",
        "    for i, data in pb:\n",
        "        inp, labels = data\n",
        "        labels = labels\n",
        "        if torch.cuda.is_available():\n",
        "            inp = send_to_device(inp)\n",
        "            labels = send_to_device(labels)\n",
        "            edges = send_to_device(edges)\n",
        "            nodes = send_to_device(nodes)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(nodes, edges, inp)\n",
        "        loss = loss_fn(outputs, labels.float())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        pb.set_description('batch {} loss: {}'.format(i + 1, loss.item()))\n",
        "        if i % 10 == 9:\n",
        "            scheduler.step()\n",
        "        del inp, labels\n",
        "        model.zero_grad()\n",
        "    return running_loss / len(train_data_loader.dataset)\n",
        "\n",
        "\n",
        "def save_model(model, save_dir=\".\"):\n",
        "    model_path = os.path.join(save_dir, model.name)\n",
        "    if not os.path.exists(model_path):\n",
        "        os.makedirs(model_path)\n",
        "    model_path = os.path.join(model_path, \"weights\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# model training\n",
        "def train_model(model, nodes, edges, train_data_loader, val_data_loader=None, epochs=30, lr=1e-5, batch_size=16, scheduler_params = None):\n",
        "    print(\"========================================================\")\n",
        "    print(f\"TRAINING {model.name}\")\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
        "    decayRate = 0.90\n",
        "    if scheduler_params == None:\n",
        "        scheduler_params = {\n",
        "            \"name\": optim.lr_scheduler.ExponentialLR,\n",
        "            \"params\": {\n",
        "                \"verbose\": True,\n",
        "                \"gamma\": 0.99\n",
        "            }\n",
        "        }\n",
        "    scheduler = scheduler_params[\"name\"](\n",
        "        optimizer=optimizer,**scheduler_params[\"params\"])\n",
        "\n",
        "    best_vloss = 1000000.\n",
        "    best_model_state_dict = None\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.to(\"cuda\")\n",
        "    model.zero_grad()\n",
        "    for epoch in range(epochs):\n",
        "        print('EPOCH {}:'.format(epoch + 1))\n",
        "\n",
        "        # Make sure gradient tracking is on, and do a pass over the data\n",
        "        model.train(True)\n",
        "        avg_loss = train_one_epoch(\n",
        "            model, nodes, edges, train_data_loader, optimizer, loss_fn, scheduler)\n",
        "        train_losses.append(avg_loss)\n",
        "        if val_data_loader is not None:\n",
        "            # We don't need gradients on to do reporting\n",
        "            model.train(False) \n",
        "            running_vloss = 0.0\n",
        "            for i, vdata in enumerate(val_data_loader):\n",
        "                vinp, vlabels = vdata\n",
        "                if torch.cuda.is_available():\n",
        "                    vinp = send_to_device(vinp)\n",
        "                    nodes = send_to_device(nodes)\n",
        "                    edges = send_to_device(edges)\n",
        "                    vlabels = send_to_device(vlabels)\n",
        "                with torch.no_grad():\n",
        "                    voutputs = model(nodes, edges, vinp)\n",
        "                    vloss = loss_fn(voutputs, vlabels.float())\n",
        "                    running_vloss += vloss\n",
        "            avg_vloss = running_vloss / (i + 1)\n",
        "            del vinp, vlabels\n",
        "            val_losses.append(avg_vloss.cpu())\n",
        "        else:\n",
        "            print(\"validation data loader not provided. Hence, validation error will be same as training error\")\n",
        "            avg_vloss = avg_loss\n",
        "\n",
        "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "        # Track best performance, and save the model's state\n",
        "        if avg_vloss < best_vloss:\n",
        "            best_vloss = avg_vloss\n",
        "            del best_model_state_dict\n",
        "            best_model_state_dict = deepcopy(model.state_dict())\n",
        "            save_model(model)\n",
        "    print(f\"the best loss is {best_vloss}\")\n",
        "    model.state_dict(best_model_state_dict)\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "def evaluate(model, nodes, edges, dataloader, labels, working_dir=\".\"):\n",
        "    print(\"========================================================\")\n",
        "    print(f\"EVALUATING {model.name}\")\n",
        "    outputs = []\n",
        "    true_classes = []\n",
        "    sigmoid = nn.Sigmoid()\n",
        "    if torch.cuda.is_available():\n",
        "        sigmoid = sigmoid.to(\"cuda\")\n",
        "    num_labels = list(range(len(labels)))\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inp, true_class = data\n",
        "        if torch.cuda.is_available():\n",
        "            inp = send_to_device(inp)\n",
        "            model = model.to(\"cuda\")\n",
        "            nodes = send_to_device(nodes)\n",
        "            edges = send_to_device(edges)\n",
        "        with torch.no_grad():\n",
        "            output = model(nodes, edges, inp)\n",
        "            output = sigmoid(output)\n",
        "        output = torch.where(output >= 0.5, 1, 0)\n",
        "        output = output.cpu().numpy()\n",
        "        outputs.extend(output.tolist())\n",
        "        true_classes.extend(true_class.tolist())\n",
        "    precision = precision_score(\n",
        "        true_classes, outputs, labels=num_labels, average=\"binary\")\n",
        "    recall = recall_score(true_classes, outputs,\n",
        "                          labels=num_labels, average=\"binary\")\n",
        "    f1 = f1_score(true_classes, outputs, labels=num_labels, average=\"binary\")\n",
        "    accuracy = accuracy_score(true_classes, outputs)\n",
        "    print(f\"precision: {precision}\")\n",
        "    print(f\"recall: {recall}\")\n",
        "    print(f\"f1 score: {f1}\")\n",
        "    print(f\"accuracy: {accuracy}\")\n",
        "    cf = confusion_matrix(true_classes, outputs, labels=num_labels)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    cf_dataframe = pd.DataFrame(cf, columns=labels, index=labels)\n",
        "    sns.heatmap(cf_dataframe, annot=True, linewidths=.5, cmap=\"YlGnBu\")\n",
        "    plt.show()\n",
        "    return (precision, recall, f1)"
      ],
      "metadata": {
        "id": "JL6m6rvkGdIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PairDecoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(PairDecoder, self).__init__()\n",
        "  \n",
        "  def forward(self, node_embeddings, inp):\n",
        "    src_embeddings = node_embeddings[inp[:, 0].long()]\n",
        "    dest_embeddings = node_embeddings[inp[:, 1].long()]\n",
        "    return torch.sum(src_embeddings * dest_embeddings, axis=1)\n",
        "\n",
        "pair_decoder = PairDecoder()\n",
        "output = pair_decoder(node_embeddings, torch.Tensor(train_target_pair_x))\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ1z-cfEKOM3",
        "outputId": "928abed6-5ae0-4297-826c-f5fe725de68a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 88.2966,  30.8720,  80.0594,  ...,  51.3117, 129.1353,  56.7068],\n",
            "       grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNModel(nn.Module):\n",
        "  def __init__(self, node_vector_shapes, layers, emb_size=256, name = 'gcn_model'):\n",
        "    super(GCNModel, self).__init__()\n",
        "    self.name = name\n",
        "    self.preprocessor = Preprocessor(node_vector_shapes, emb_size=emb_size)\n",
        "    layers = [emb_size] + layers\n",
        "    self.gcn_encoder = GCNEncoder(layers)\n",
        "    self.pair_decoder = PairDecoder()\n",
        "  \n",
        "  def forward(self, nodes, edges, inp):\n",
        "    preprocessed_nodes = self.preprocessor(nodes)\n",
        "    node_embeddings = self.gcn_encoder(preprocessed_nodes, edges)\n",
        "    return self.pair_decoder(node_embeddings, inp)\n",
        "\n",
        "gcn_model = GCNModel(node_vector_shapes, layers=[512, 256, 128], emb_size=256)\n",
        "gcn_model(node_vector_list, edges, torch.Tensor(train_target_pair_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StJo2pI4NY5_",
        "outputId": "71f37d08-09ad-430f-8c8f-3693fab357af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 50.9296,  35.5994,  53.6756,  ...,  52.5949, 114.8929,  73.8575],\n",
              "       grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gcn_model = GCNModel(node_vector_shapes, layers=[512, 256, 128], emb_size=256)\n",
        "train_target_dataset = PairDataset(train_target_pair_x,train_target_pair_y)\n",
        "train_target_dataloader = data.DataLoader(train_target_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_target_dataset = PairDataset(test_target_pair_x,test_target_pair_y)\n",
        "test_target_dataloader = data.DataLoader(test_target_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "scheduler_params = {\n",
        "            \"name\": optim.lr_scheduler.ExponentialLR,\n",
        "            \"params\": {\n",
        "                \"verbose\": False,\n",
        "                \"gamma\": 0.99\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "train_losses, val_losses = train_model(gcn_model, node_vector_list, edges, train_target_dataloader, test_target_dataloader,\n",
        "                                       epochs=50, scheduler_params = scheduler_params, batch_size = 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQTWUR6xJ6V2",
        "outputId": "37941e67-6eb4-43f8-d25c-1695d2219a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================\n",
            "TRAINING gcn_model\n",
            "EPOCH 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 1.2077088356018066: : 169it [00:12, 13.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.06345278734804768 valid 1.4898864030838013\n",
            "EPOCH 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.7294897437095642: : 169it [00:04, 39.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.016260392415826136 valid 0.9668117761611938\n",
            "EPOCH 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.5427905321121216: : 169it [00:04, 40.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.011375310756397266 valid 0.8124424815177917\n",
            "EPOCH 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.8553263545036316: : 169it [00:04, 40.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.009447785473673878 valid 0.7418997287750244\n",
            "EPOCH 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.40909668803215027: : 169it [00:04, 40.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.008335137656602194 valid 0.7011764645576477\n",
            "EPOCH 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.527130663394928: : 169it [00:06, 24.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.007644248169970906 valid 0.6908693313598633\n",
            "EPOCH 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.4360870122909546: : 169it [00:05, 29.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.00714140031013927 valid 0.6606795787811279\n",
            "EPOCH 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.48658108711242676: : 169it [00:05, 29.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.0067849064317261135 valid 0.646618127822876\n",
            "EPOCH 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.6404510736465454: : 169it [00:06, 27.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.006507926988274781 valid 0.6465071439743042\n",
            "EPOCH 10:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.4358961582183838: : 169it [00:05, 33.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.006286114836669044 valid 0.6639609336853027\n",
            "EPOCH 11:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.2013590782880783: : 169it [00:04, 40.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.006103771549751521 valid 0.626664400100708\n",
            "EPOCH 12:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.3565713167190552: : 169it [00:04, 39.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.0059638572237282654 valid 0.627898633480072\n",
            "EPOCH 13:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.3444306552410126: : 169it [00:04, 40.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005849817783960219 valid 0.637347400188446\n",
            "EPOCH 14:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.40323376655578613: : 169it [00:06, 27.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.0057575329337514105 valid 0.6130871176719666\n",
            "EPOCH 15:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.30877622961997986: : 169it [00:06, 26.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.0056736191207664155 valid 0.6093601584434509\n",
            "EPOCH 16:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.296509325504303: : 169it [00:06, 25.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005607361873697744 valid 0.6196402311325073\n",
            "EPOCH 17:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.3838789463043213: : 169it [00:04, 36.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005553610448939327 valid 0.6153740286827087\n",
            "EPOCH 18:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.32788145542144775: : 169it [00:05, 31.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005506641275135703 valid 0.6079737544059753\n",
            "EPOCH 19:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.34322044253349304: : 169it [00:06, 27.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.0054696377464106775 valid 0.6086909174919128\n",
            "EPOCH 20:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.453158974647522: : 169it [00:04, 40.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005439860315892307 valid 0.6042482852935791\n",
            "EPOCH 21:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.24244904518127441: : 169it [00:04, 40.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005405052111715172 valid 0.6032735109329224\n",
            "EPOCH 22:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.36283519864082336: : 169it [00:06, 26.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005386103080592156 valid 0.6094936728477478\n",
            "EPOCH 23:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.4056451916694641: : 169it [00:04, 40.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005367420539218496 valid 0.6123618483543396\n",
            "EPOCH 24:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.36405542492866516: : 169it [00:04, 41.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005349964037336021 valid 0.6107862591743469\n",
            "EPOCH 25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.3012268841266632: : 169it [00:05, 33.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005333220419484817 valid 0.6055868864059448\n",
            "EPOCH 26:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.23340731859207153: : 169it [00:04, 38.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.00531899249700566 valid 0.6062564849853516\n",
            "EPOCH 27:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.30335795879364014: : 169it [00:05, 30.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005310700971295663 valid 0.6032342910766602\n",
            "EPOCH 28:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.2155042439699173: : 169it [00:05, 30.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.0052998197344866025 valid 0.6099045872688293\n",
            "EPOCH 29:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.3089357912540436: : 169it [00:04, 35.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005295424395818412 valid 0.5990433096885681\n",
            "EPOCH 30:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.4625052511692047: : 169it [00:04, 37.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005293790168876623 valid 0.6169752478599548\n",
            "EPOCH 31:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.3948391079902649: : 169it [00:05, 31.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005286431877739411 valid 0.6002269387245178\n",
            "EPOCH 32:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.36842405796051025: : 169it [00:05, 31.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005281038172315265 valid 0.6107208132743835\n",
            "EPOCH 33:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.4301013648509979: : 169it [00:06, 28.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005279245785160906 valid 0.6049257516860962\n",
            "EPOCH 34:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.2951202392578125: : 169it [00:05, 30.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005271681751976505 valid 0.6033490896224976\n",
            "EPOCH 35:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.36172565817832947: : 169it [00:05, 28.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005271023868608855 valid 0.6117686033248901\n",
            "EPOCH 36:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.32450026273727417: : 169it [00:05, 30.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005267478734094698 valid 0.6093476414680481\n",
            "EPOCH 37:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.3843969702720642: : 169it [00:05, 32.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005267426250090042 valid 0.604564368724823\n",
            "EPOCH 38:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.3626234531402588: : 169it [00:04, 35.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005265068996553402 valid 0.6318264603614807\n",
            "EPOCH 39:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.41163209080696106: : 169it [00:04, 40.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.0052651187499209605 valid 0.6107187271118164\n",
            "EPOCH 40:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.36833009123802185: : 169it [00:04, 37.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005262476841962092 valid 0.6074361801147461\n",
            "EPOCH 41:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.4320187568664551: : 169it [00:04, 41.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005263445462996327 valid 0.6002524495124817\n",
            "EPOCH 42:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.2787582278251648: : 169it [00:06, 27.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005257669152242951 valid 0.6014639139175415\n",
            "EPOCH 43:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.2726059556007385: : 169it [00:06, 27.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.0052567244594861826 valid 0.6072916388511658\n",
            "EPOCH 44:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.25323083996772766: : 169it [00:04, 40.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005255476914502215 valid 0.6120681762695312\n",
            "EPOCH 45:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.5208153128623962: : 169it [00:04, 40.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005263469667374613 valid 0.6143167614936829\n",
            "EPOCH 46:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.2818273603916168: : 169it [00:04, 35.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005255439240545482 valid 0.6010787487030029\n",
            "EPOCH 47:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.2783249616622925: : 169it [00:04, 40.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005255024111920294 valid 0.6050530076026917\n",
            "EPOCH 48:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.4120039641857147: : 169it [00:04, 41.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.00525899585671219 valid 0.6140750050544739\n",
            "EPOCH 49:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.303337961435318: : 169it [00:04, 40.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.005255350523830366 valid 0.6096916198730469\n",
            "EPOCH 50:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch 169 loss: 0.2767138183116913: : 169it [00:04, 40.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS train 0.0052543592307773895 valid 0.6048410534858704\n",
            "the best loss is 0.5990433096885681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1366: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
            "  \"Positional args are being deprecated, use kwargs instead. Refer to \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(gcn_model, node_vector_list, edges, test_target_dataloader, [0, 1])\n",
        "plt.figure()\n",
        "plt.plot(train_losses)\n",
        "plt.plot([l.cpu() for l in val_losses])\n",
        "plt.legend([\"train\",\"val\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "pFhpY9agPqy8",
        "outputId": "ed8538ed-d609-4638-8405-ea88446e7801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================\n",
            "EVALUATING gcn_model\n",
            "precision: 0.7000753579502638\n",
            "recall: 0.7994836488812392\n",
            "f1 score: 0.7464845319405384\n",
            "accuracy: 0.7662097072989996\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_node_vectors = gcn_model.preprocessor(send_to_device(node_vector_list))\n",
        "node_embeddings = gcn_model.gcn_encoder(preprocessed_node_vectors, send_to_device(edges))\n",
        "print(node_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iGRFiTxZl04",
        "outputId": "cfc0cb86-cff7-4088-f0d6-a8f43b8137fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3440, -0.0099, -0.9674,  ..., -0.9776, -0.0922, -0.8979],\n",
            "        [ 0.7179,  0.1215, -0.4239,  ..., -0.2927,  0.1936,  0.0482],\n",
            "        [ 0.6437, -0.1012,  0.5518,  ...,  0.1124, -0.7340, -0.1344],\n",
            "        ...,\n",
            "        [ 0.1441,  0.5823,  0.1317,  ...,  0.4407,  0.5274, -0.0348],\n",
            "        [ 0.0606,  0.6932, -0.3479,  ..., -0.3015,  0.3388, -0.1991],\n",
            "        [ 0.1153,  0.0676,  0.3040,  ...,  0.2021, -0.1536,  0.0445]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "agglomerative_algo = AgglomerativeClustering(n_clusters=None, affinity= 'cosine', linkage='average', distance_threshold = 0.9)\n",
        "clusters = agglomerative_algo.fit_predict(node_embeddings.cpu().detach())"
      ],
      "metadata": {
        "id": "WLLD8r5T0_Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIC4cayN1dtV",
        "outputId": "5412013d-b408-4372-bb55-e7d9959ab032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = pd.DataFrame(list(zip(index_decoder, clusters)), columns = ['name', 'cluster'])\n",
        "result_df.to_csv(\"cluster_unsupervised.csv\")"
      ],
      "metadata": {
        "id": "8uT9XC1j1qDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_RELATIONS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjq-_rb9Hv4Y",
        "outputId": "0b8120ac-1f2a-496d-c6ec-496c6b03e1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6207"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "includes, fieldnames = get_data(os.path.join(root_path, \"rel_INCLUDES_exercise.csv\"), convert_to_dict=False)\n",
        "workout_sessions = [row['workout_name'] for row in includes]\n"
      ],
      "metadata": {
        "id": "Uy34WVHh2DSN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}